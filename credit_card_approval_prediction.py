# -*- coding: utf-8 -*-
"""Credit Card Approval Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FbjKGgIEDZGLFSRFz0TExDdMvs8DOo8C

# **A Credit Card Dataset for Machine Learning!**
## **By: Shrouk Abdelhameed Elmasry**

##**Context**
>#### **The goal of this project is to build a model to predict whether a customer's credit is good or bad based on their application data.**
>#### **The label good/bad in the data were not pre-defined. Instead, only credit history of past customers were given.**

##**Table of Contents**
This end-to-end project is divided into 3 parts:
1. Explanatory Data Analysis (EDA) & Feature Engineering
2. Feature Scaling and Selection (Imbalanced Data Handling)
3. Machine Learning Modelling (Classification)

##**1. Explanatory Data Analysis (EDA) & Feature Engineering**
#Letâ€™s import the necessary libraries:
"""

import numpy as np
import pandas as pd 
import seaborn as sns
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE
from plotly.subplots import make_subplots
import plotly.graph_objects as go
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score,f1_score,plot_confusion_matrix

"""# 2. Importing Data & EDA"""

data = pd.read_csv("application_record.csv", encoding = 'utf-8') 
record = pd.read_csv("credit_record.csv", encoding = 'utf-8')

print("The Total Number of datapoints for application records: {}".format(len(data)))
print("The Number of unique clients of them: {}".format(len(data.ID.unique())))
data.head()

"""We can notice that the Unique clients and total num of rows are not equal,which means there are duplicates."""

print("Number of datapoints for credit records: {}".format(len(record)))
print("Number of unique clients in dataset: {}".format(len(record.ID.unique())))
record.head()

"""The common between the two dataframes (the intersection cases between 2 datasets)"""

len(set(record['ID']).intersection(set(data['ID']))) # checking to see how many records match in two datasets

"""The number of unique ids in the two datasets is not equal. There are fewer customers than applications in the credit record dataset. The intersection is 36,457 customers.

# **Check Missing Values**
"""

data.isna().sum()

record.isna().sum()

"""We have checked the null values for records data, and all good here.

# **Unique counts**
"""

unique_counts = pd.DataFrame.from_records([(col, data[col].nunique()) for col in data.columns],
                          columns=['Column_Name', 'Num_Unique']).sort_values(by=['Num_Unique'])
unique_counts

unique_counts = pd.DataFrame.from_records([(col, record[col].nunique()) for col in record.columns],
                          columns=['Column_Name', 'Num_Unique']).sort_values(by=['Num_Unique'])
unique_counts

"""# **Data Visualization**"""

sns.set_context("notebook",font_scale=.7,rc={"grid.linewidth": 0.1,'patch.linewidth': 0.0,
    "axes.grid":True,
    "grid.linestyle": "-",
    "axes.titlesize" : 13,                                       
    "figure.autolayout":True})
                
palette_1 = ['#FF5E5B','#EC9B9A','#00CECB','#80DE99','#C0E680','#FFED66']

sns.set_palette(sns.color_palette(sns.color_palette(palette_1)))

plt.figure(figsize=(10,10))

cols_to_plot = ["CNT_CHILDREN","AMT_INCOME_TOTAL","DAYS_BIRTH","DAYS_EMPLOYED"]
data[cols_to_plot].hist(edgecolor='black', linewidth=1.2)
fig=plt.gcf()
fig.set_size_inches(12,6)

"""There are outliers in 2 features.

* CNT_CHILDREN
* AMT_INCOME_TOTAL
"""

fig, axes = plt.subplots(1,3)
fig.set_size_inches(10,5)

g1= data['CODE_GENDER'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True, colors=["#9eb9f3","#ffb7d0"],textprops = {'fontsize':8}, ax=axes[0])
g1.set_title("Customer Distribution by Gender")

g2= data['FLAG_OWN_CAR'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True,colors=["#dcb0f2","#87c55f"],textprops = {'fontsize':8}, ax=axes[1])
g2.set_title("Car Ownership")

g3= data['FLAG_OWN_REALTY'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True,colors=["#76B5B3","#00CECB"],textprops = {'fontsize':8}, ax=axes[2])
g3.set_title("Realty Ownership")


plt.tight_layout()

plt.show()

"""# **2. Feature Scaling and Selection (Imbalanced Data Handling)**

"""

data = data.drop_duplicates('ID', keep='last') #remove duplicate values and keep the last entry of the ID if its repeated.
data.drop('OCCUPATION_TYPE', axis=1, inplace=True) #the occupation type has missing values, we dropped them.

object_columns = data.columns[data.dtypes =='object'].tolist() #object columns in dataset

unique_counts = pd.DataFrame.from_records([(col, data[object_columns][col].nunique()) for col in data[object_columns].columns],
                          columns=['Column_Name', 'Num_Unique']).sort_values(by=['Num_Unique'])

unique_counts #unique counts for object columns

"""For the columns that have non numeric values, We will convert them numeric to see if they are useful.



"""

#renaming columns

data.rename(columns={"CODE_GENDER":"Gender","FLAG_OWN_CAR":"Own_Car","FLAG_OWN_REALTY":"Own_Realty",
                     "CNT_CHILDREN":"Children_Count","AMT_INCOME_TOTAL":"Income","NAME_EDUCATION_TYPE":"Education",
                     "NAME_FAMILY_STATUS":"Family_Status","NAME_HOUSING_TYPE":"Housing_Type","DAYS_BIRTH":"Birthday",
                     "DAYS_EMPLOYED":"Employment_Date","FLAG_MOBIL":"Own_Mobile","FLAG_WORK_PHONE":"Own_Work_Phone",
                     "FLAG_PHONE":"Own_Phone","FLAG_EMAIL":"Own_Email","CNT_FAM_MEMBERS":"Family_Member_Count",
                    "NAME_INCOME_TYPE":"Income_Type"},inplace=True)

# all users account open month

# => smallest value of MONTHS_BALANCE, is the month when loan was granted
open_month=pd.DataFrame(record.groupby(["ID"])["MONTHS_BALANCE"].agg(min))
open_month=open_month.rename(columns={'MONTHS_BALANCE':'begin_month'}) 
customer_data=pd.merge(data,open_month,how="left",on="ID") # merge to record data

#convert categoric features into numeric
customer_data["Gender"] =  customer_data['Gender'].replace(['F','M'],[0,1])
customer_data["Own_Car"] = customer_data["Own_Car"].replace(["Y","N"],[1,0])
customer_data["Own_Realty"] = customer_data["Own_Realty"].replace(["Y","N"],[1,0])
customer_data["Is_Working"] = customer_data["Income_Type"].replace(["Working","Commercial associate","State servant","Pensioner","Student"],[1,1,1,0,0])
customer_data["In_Relationship"] = customer_data["Family_Status"].replace(["Civil marriage","Married","Single / not married","Separated","Widow"],[1,1,0,0,0])

housing_type = {'House / apartment' : 'House / apartment', 'With parents': 'With parents',
                    'Municipal apartment' : 'House / apartment', 'Rented apartment': 'House / apartment',
                    'Office apartment': 'House / apartment', 'Co-op apartment': 'House / apartment'}
customer_data["Housing_Type"] = customer_data['Housing_Type'].map(housing_type)

family_status = {'Single / not married':'Single', 'Separated':'Single',
                     'Widow':'Single', 'Civil marriage':'Married', 'Married':'Married'}
customer_data["Family_Status"] = customer_data["Family_Status"].map(family_status)

education_type = {'Secondary / secondary special':'secondary',
                     'Lower secondary':'secondary', 'Higher education':'Higher education',
                     'Incomplete higher':'Higher education', 'Academic degree':'Academic degree'}
customer_data["Education"] = customer_data["Education"].map(education_type)

income_type = {'Commercial associate':'Working',
                  'State servant':'Working',  'Working':'Working',
                  'Pensioner':'Pensioner', 'Student':'Student'}

customer_data["Income_Type"] = customer_data["Income_Type"].map(income_type)
customer_data["Household_Size"] = customer_data["Children_Count"] + customer_data["In_Relationship"].apply(lambda x: 2 if x==1 else 1)
customer_data["Age"] = round((customer_data.Birthday/365)*-1)
customer_data["Experience"] = customer_data.Employment_Date/365
customer_data['Experience']=customer_data['Experience'].apply(lambda v : int(v*-1) if v <0 else 0)

customer_data=customer_data.drop(columns=['Employment_Date','Birthday','Children_Count'])
customer_data= pd.get_dummies(customer_data, columns=['Income_Type', 'Education','Family_Status',"Housing_Type"])

customer_data.head()

"""We will look at numeric columns to see if there is anything that needs to be changed."""

from plotly.subplots import make_subplots
import plotly.graph_objects as go

other_numerical_cols = ["Income","Age","Experience","Household_Size"]

fig = make_subplots(rows=2, cols=2, start_cell="bottom-left",
                   subplot_titles=("Income", "Age", "Experience", "Family Member Count"))

fig.add_trace(go.Box(x=customer_data.Income, name='Income',boxmean=True),row=1,col=1)
fig.add_trace(go.Box(x=customer_data.Age, name='Age', boxmean=True), row=1, col=2)
fig.add_trace(go.Box(x=customer_data.Experience, name='Experience', boxmean=True), row=2, col=1)
fig.add_trace(go.Box(x=customer_data.Household_Size, name="Family Member Count", boxmean=True),row=2, col=2)

fig.show()

"""As seen above, there are some outliers values in children count, family member count, income and employment rate columns

* We need to remove these outliers to make sure they do not affect our model results.
* We will now remove these outliers by using z scores.
"""

def calculate_z_scores(df, cols):
    for col in cols:
        df[col+"_z_score"] = (df[col] - df[col].mean())/df[col].std()
    return df

df_2 = calculate_z_scores(df = customer_data, cols = ["Income","Experience","Household_Size"])


#removing outliers
filter_2 = df_2.Household_Size_z_score.abs() <= 3.5
filter_3 = df_2.Experience_z_score.abs() <= 3.5
filter_4 = df_2.Income_z_score.abs() <= 3.5

customer_apps = df_2[filter_2 & filter_3 & filter_4]

customer_apps.drop(columns= ["Income_z_score","Experience_z_score","Household_Size_z_score"],inplace=True)

other_numerical_cols = ["Income","Age","Experience","Family_Member_Count"]

fig = make_subplots(rows=2, cols=2, start_cell="bottom-left",
                   subplot_titles=("Income", "Age", "Experience", "Family Member Count"))

fig.add_trace(go.Box(x=customer_apps.Income, name='Income',boxmean=True),row=1,col=1)
fig.add_trace(go.Box(x=customer_apps.Age, name='Age', boxmean=True), row=1, col=2)
fig.add_trace(go.Box(x=customer_apps.Experience, name='Experience', boxmean=True), row=2, col=1)
fig.add_trace(go.Box(x=customer_apps.Household_Size, name="Family Member Count", boxmean=True),row=2, col=2)

fig.show()

record['dep_value'] = None
record['dep_value'][record['STATUS'] =='2']='Yes'  # 2: 60-89 days overdue 
record['dep_value'][record['STATUS'] =='3']='Yes'  # 3: 90-119 days overdue
record['dep_value'][record['STATUS'] =='4']='Yes'  # 4: 120-149 days overdue 
record['dep_value'][record['STATUS'] =='5']='Yes'  # 5: Overdue or bad debts, write-offs for more than 150 days 

record_count=record.groupby('ID').count() #num of days for each applicant
record_count['dep_value'][record_count['dep_value'] > 0]='Yes' 
record_count['dep_value'][record_count['dep_value'] == 0]='No' 
record_count = record_count[['dep_value']]

# Data frame to analyze length of time since initial approval of credit card
# Shows number of past dues, paid off and no loan status.
grouped = record.groupby('ID')

pivot_tb = record.pivot(index = 'ID', columns = 'MONTHS_BALANCE', values = 'STATUS')
pivot_tb['open_month'] = grouped['MONTHS_BALANCE'].min() # smallest value of MONTHS_BALANCE, is the month when loan was granted
pivot_tb['end_month'] = grouped['MONTHS_BALANCE'].max() # biggest value of MONTHS_BALANCE, might be observe over or canceling account
pivot_tb['window'] = pivot_tb['end_month'] - pivot_tb['open_month'] # calculate observe window
pivot_tb['window'] += 1 # Adding 1 since month starts at 0.

#Counting number of past dues, paid offs and no loans.
pivot_tb['paid_off'] = pivot_tb[pivot_tb.iloc[:,0:61] == 'C'].count(axis = 1) # new column contains paid offs count
pivot_tb['pastdue_1-29'] = pivot_tb[pivot_tb.iloc[:,0:61] == '0'].count(axis = 1)
pivot_tb['pastdue_30-59'] = pivot_tb[pivot_tb.iloc[:,0:61] == '1'].count(axis = 1)
pivot_tb['pastdue_60-89'] = pivot_tb[pivot_tb.iloc[:,0:61] == '2'].count(axis = 1)
pivot_tb['pastdue_90-119'] = pivot_tb[pivot_tb.iloc[:,0:61] == '3'].count(axis = 1)
pivot_tb['pastdue_120-149'] = pivot_tb[pivot_tb.iloc[:,0:61] == '4'].count(axis = 1)
pivot_tb['pastdue_over_150'] = pivot_tb[pivot_tb.iloc[:,0:61] == '5'].count(axis = 1)
pivot_tb['no_loan'] = pivot_tb[pivot_tb.iloc[:,0:61] == 'X'].count(axis = 1) # no loans count column 
#Setting Id column to merge with app data.
pivot_tb['ID'] = pivot_tb.index

pivot_tb

target = pd.DataFrame() 
target['ID'] = pivot_tb.index
target['paid_off'] = pivot_tb['paid_off'].values
target['#_of_pastdues'] = pivot_tb['pastdue_1-29'].values+ pivot_tb['pastdue_30-59'].values + pivot_tb['pastdue_60-89'].values +pivot_tb['pastdue_90-119'].values+pivot_tb['pastdue_120-149'].values +pivot_tb['pastdue_over_150'].values
target['no_loan'] = pivot_tb['no_loan'].values
customer_apps_1 = customer_apps.merge(target, how = 'inner', on = 'ID')

customer_apps_2=pd.merge(customer_apps_1,record_count,how='inner',on='ID')
customer_apps_2['target']=customer_apps_2['dep_value']
customer_apps_2.loc[customer_apps_2['target']=='Yes','target']=1
customer_apps_2.loc[customer_apps_2['target']=='No','target']=0

customer_apps_2.drop(columns=["dep_value"],inplace=True)

matplotlib.rcParams.update(matplotlib.rcParamsDefault)

f, ax = plt.subplots(figsize=(15,15))
cmap = sns.diverging_palette(230, 20, as_cmap=True)
corr = customer_apps_2.drop(columns=["Own_Mobile"]).corr().round(1)
mask = np.triu(np.ones_like(corr, dtype=bool))
sns.heatmap(corr, annot=True, mask = mask, cmap=cmap)

customer_apps_2['target'].value_counts().plot.pie(explode=[0.1,0.1],autopct='%1.1f%%',shadow=True, colors=['#FF5E5B', '#C0E680'],textprops = {'fontsize':7}).set_title("Target distribution")

plt.show()

"""#**Separate predictors and the target**"""

X = customer_apps_2.drop(columns=['ID','target'])
y = customer_apps_2['target']

"""#**Scaling**"""

scaler = StandardScaler()
X = scaler.fit_transform(X)

"""#**Handling unbalance data problem**

I have used Synthetic Minority Over-Sampling Technique(SMOTE) to overcome sample imbalance problem.

"""

y = y.astype('int')
sm = SMOTE(random_state=42) 
X_res, y_res = sm.fit_resample(X, y)

"""#3. Machine Learning Modelling (Classification)

###  XGBoost and CatBoost algorithms are performed.

###**XGBoost** It is a decision-tree-based ensemble Machine Learning algorithm that uses a gradient boosting framework. Execution speed and high performance are the main reasons to use XGBoost.


###**CatBoost** is an open source algorithm based on gradient boosted decision trees. It supports numerical, categorical and text features. It works well with heterogeneous data and even relatively small data.
"""

# Split the data
X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, random_state=42, test_size=0.2)

"""##**1. XGBoost:**"""

model1 = XGBClassifier(max_depth=12,n_estimators=250, min_child_weight=8, subsample=0.8, learning_rate =0.02, seed=42, use_label_encoder=False)
model1.fit(X_train,y_train)
y_pred_train = model1.predict(X_train)
y_pred_test = model1.predict(X_test)

print('F-Score On Training Set : ', f1_score(y_train, y_pred_train))
print('Accuracy On Training Set : ', accuracy_score(y_train, y_pred_train))

print('F-Score On Test Set : ',f1_score(y_test, y_pred_test))
print('Accuracy On Test Set : ',accuracy_score(y_test, y_pred_test))

plot_confusion_matrix(model1, X_test, y_test, cmap='PuBu_r')
plt.show()

"""##**2. CatBoost:**"""

from catboost import CatBoostClassifier
model2 = CatBoostClassifier(iterations=250,learning_rate=0.2,od_type='Iter', verbose=25,depth=16,random_seed=42)

model2.fit(X_train,y_train)
y_pred_train = model2.predict(X_train)
y_pred_test = model2.predict(X_test)

print('F-Score On Training Set : ', f1_score(y_train, y_pred_train))
print('Accuracy On Training Set : ', accuracy_score(y_train, y_pred_train))

print('F-Score On Test Set : ',f1_score(y_test, y_pred_test))
print('Accuracy On Test Set : ',accuracy_score(y_test, y_pred_test))

plot_confusion_matrix(model2, X_test, y_test, cmap='PuBu_r')
plt.show()